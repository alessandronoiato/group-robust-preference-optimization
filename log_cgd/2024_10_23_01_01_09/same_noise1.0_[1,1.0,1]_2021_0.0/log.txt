2024-10-23 01:01:13,357 - /Users/noesis/Code/group-robust-preference-optimization/experiments/run_glb_noisy.py[line:215] - INFO: Logging to log_cgd/2024_10_23_01_01_09/same_noise1.0_[1,1.0,1]_2021_0.0
2024-10-23 01:01:13,359 - /Users/noesis/Code/group-robust-preference-optimization/experiments/run_glb_noisy.py[line:216] - INFO: (IB) Seed: 2021
2024-10-23 01:01:13,359 - /Users/noesis/Code/group-robust-preference-optimization/experiments/run_glb_noisy.py[line:217] - INFO: (IB) Data: 300
2024-10-23 01:01:13,495 - /Users/noesis/Code/group-robust-preference-optimization/experiments/run_glb_noisy.py[line:316] - INFO: MLE reward loss: 0.0002, l2 distance: 2194.4385, acc: 1.00.
2024-10-23 01:01:13,496 - /Users/noesis/Code/group-robust-preference-optimization/experiments/run_glb_noisy.py[line:317] - INFO: True reward parameter: [[2. 2. 2. 2.]
 [2. 2. 2. 2.]
 [2. 2. 2. 2.]]
2024-10-23 01:01:13,497 - /Users/noesis/Code/group-robust-preference-optimization/experiments/run_glb_noisy.py[line:318] - INFO: MLE reward parameter: [620.22118185 720.0685001  648.99839531 539.36401535]
2024-10-23 01:01:13,706 - /Users/noesis/Code/group-robust-preference-optimization/experiments/run_glb_noisy.py[line:334] - INFO: Learned oracle reward: 2.7737, 2.7737, 2.7737
2024-10-23 01:01:15,338 - /Users/noesis/Code/group-robust-preference-optimization/algos/linear_bandit/common_gradient_descent.py[line:386] - INFO: Iteration: 0, train_loss: 0.6450, val_loss: 0.6492, grad_norm:  1.3304, live_grad: 0.0000, reward_err: 0.0281, 0.0281, 0.0281, KL_dist: 0.1079, 0.1079, 0.1079, param: [0.7546966075897217, 2.6114654541015625, 3.765254020690918, 3.629005193710327], weights: [0.338712602853775, 0.36092400550842285, 0.3003634214401245], train_wt_loss:  0.6614, val_wt_loss: 0.6512, train_grp_loss: [16.650333404541016, 18.267295837402344, 15.466714859008789], val_grp_loss: [18.560035705566406, 17.684253692626953, 17.382844924926758], max_reward_err: 0.0281, max_reward_err_index: 0, max_kl_dist: 0.1079, max_kl_dist_index: 0, max_train_grp_loss: 18.2673, max_train_grp_loss_index: 1, max_val_grp_loss: 18.5600, max_val_grp_loss_index: 0
2024-10-23 01:01:16,215 - /Users/noesis/Code/group-robust-preference-optimization/algos/linear_bandit/common_gradient_descent.py[line:386] - INFO: Iteration: 1, train_loss: 0.6438, val_loss: 0.6481, grad_norm:  1.3133, live_grad: 0.0000, reward_err: 0.0268, 0.0268, 0.0268, KL_dist: 0.1076, 0.1076, 0.1076, param: [0.8567237854003906, 2.6548144817352295, 3.816556692123413, 3.677248239517212], weights: [0.34202277660369873, 0.3888012170791626, 0.26917600631713867], train_wt_loss:  0.6757, val_wt_loss: 0.6519, train_grp_loss: [16.457965850830078, 18.096216201782227, 15.3231782913208], val_grp_loss: [18.392105102539062, 17.519512176513672, 17.207609176635742], max_reward_err: 0.0268, max_reward_err_index: 0, max_kl_dist: 0.1076, max_kl_dist_index: 0, max_train_grp_loss: 18.0962, max_train_grp_loss_index: 1, max_val_grp_loss: 18.3921, max_val_grp_loss_index: 0
2024-10-23 01:01:17,042 - /Users/noesis/Code/group-robust-preference-optimization/algos/linear_bandit/common_gradient_descent.py[line:386] - INFO: Iteration: 2, train_loss: 0.6426, val_loss: 0.6470, grad_norm:  1.2965, live_grad: 0.0000, reward_err: 0.0256, 0.0256, 0.0256, KL_dist: 0.1076, 0.1076, 0.1076, param: [0.9582099914550781, 2.6970486640930176, 3.8667404651641846, 3.7242321968078613], weights: [0.3432914614677429, 0.4167623519897461, 0.23994621634483337], train_wt_loss:  0.6890, val_wt_loss: 0.6526, train_grp_loss: [16.271482467651367, 17.929323196411133, 15.185500144958496], val_grp_loss: [18.22867774963379, 17.359346389770508, 17.037385940551758], max_reward_err: 0.0256, max_reward_err_index: 0, max_kl_dist: 0.1076, max_kl_dist_index: 0, max_train_grp_loss: 17.9293, max_train_grp_loss_index: 1, max_val_grp_loss: 18.2287, max_val_grp_loss_index: 0
2024-10-23 01:01:17,873 - /Users/noesis/Code/group-robust-preference-optimization/algos/linear_bandit/common_gradient_descent.py[line:386] - INFO: Iteration: 3, train_loss: 0.6415, val_loss: 0.6459, grad_norm:  1.2799, live_grad: 0.0000, reward_err: 0.0249, 0.0249, 0.0249, KL_dist: 0.1078, 0.1078, 0.1078, param: [1.0590839385986328, 2.738236665725708, 3.9158518314361572, 3.7700178623199463], weights: [0.34259337186813354, 0.44461533427238464, 0.21279127895832062], train_wt_loss:  0.7012, val_wt_loss: 0.6533, train_grp_loss: [16.090761184692383, 17.76655387878418, 15.05349349975586], val_grp_loss: [18.069690704345703, 17.20367431640625, 16.8720760345459], max_reward_err: 0.0249, max_reward_err_index: 0, max_kl_dist: 0.1078, max_kl_dist_index: 0, max_train_grp_loss: 17.7666, max_train_grp_loss_index: 1, max_val_grp_loss: 18.0697, max_val_grp_loss_index: 0
2024-10-23 01:01:18,724 - /Users/noesis/Code/group-robust-preference-optimization/algos/linear_bandit/common_gradient_descent.py[line:386] - INFO: Iteration: 4, train_loss: 0.6403, val_loss: 0.6448, grad_norm:  1.2633, live_grad: 0.0000, reward_err: 0.0241, 0.0241, 0.0241, KL_dist: 0.1083, 0.1083, 0.1083, param: [1.159272313117981, 2.778447389602661, 3.9639365673065186, 3.8146674633026123], weights: [0.3400433361530304, 0.47218284010887146, 0.18777377903461456], train_wt_loss:  0.7124, val_wt_loss: 0.6538, train_grp_loss: [15.915680885314941, 17.607858657836914, 14.926983833312988], val_grp_loss: [17.915075302124023, 17.052419662475586, 16.711593627929688], max_reward_err: 0.0241, max_reward_err_index: 0, max_kl_dist: 0.1083, max_kl_dist_index: 0, max_train_grp_loss: 17.6079, max_train_grp_loss_index: 1, max_val_grp_loss: 17.9151, max_val_grp_loss_index: 0
2024-10-23 01:01:19,551 - /Users/noesis/Code/group-robust-preference-optimization/algos/linear_bandit/common_gradient_descent.py[line:386] - INFO: Iteration: 5, train_loss: 0.6392, val_loss: 0.6438, grad_norm:  1.2469, live_grad: 0.0000, reward_err: 0.0235, 0.0235, 0.0235, KL_dist: 0.1091, 0.1091, 0.1091, param: [1.258702039718628, 2.8177490234375, 4.0110392570495605, 3.858242988586426], weights: [0.3357881009578705, 0.4993055760860443, 0.1649063229560852], train_wt_loss:  0.7226, val_wt_loss: 0.6543, train_grp_loss: [15.746124267578125, 17.453182220458984, 14.805787086486816], val_grp_loss: [17.7647647857666, 16.90550422668457, 16.555845260620117], max_reward_err: 0.0235, max_reward_err_index: 0, max_kl_dist: 0.1091, max_kl_dist_index: 0, max_train_grp_loss: 17.4532, max_train_grp_loss_index: 1, max_val_grp_loss: 17.7648, max_val_grp_loss_index: 0
2024-10-23 01:01:20,371 - /Users/noesis/Code/group-robust-preference-optimization/algos/linear_bandit/common_gradient_descent.py[line:386] - INFO: Iteration: 6, train_loss: 0.6381, val_loss: 0.6428, grad_norm:  1.2306, live_grad: 0.0000, reward_err: 0.0222, 0.0222, 0.0222, KL_dist: 0.1100, 0.1100, 0.1100, param: [1.3573023080825806, 2.856208324432373, 4.057203769683838, 3.900806188583374], weights: [0.3299974501132965, 0.5258439779281616, 0.14415854215621948], train_wt_loss:  0.7317, val_wt_loss: 0.6547, train_grp_loss: [15.5819730758667, 17.302465438842773, 14.689727783203125], val_grp_loss: [17.618698120117188, 16.762845993041992, 16.404741287231445], max_reward_err: 0.0222, max_reward_err_index: 0, max_kl_dist: 0.1100, max_kl_dist_index: 0, max_train_grp_loss: 17.3025, max_train_grp_loss_index: 1, max_val_grp_loss: 17.6187, max_val_grp_loss_index: 0
2024-10-23 01:01:21,194 - /Users/noesis/Code/group-robust-preference-optimization/algos/linear_bandit/common_gradient_descent.py[line:386] - INFO: Iteration: 7, train_loss: 0.6370, val_loss: 0.6417, grad_norm:  1.2144, live_grad: 0.0000, reward_err: 0.0218, 0.0218, 0.0218, KL_dist: 0.1112, 0.1112, 0.1112, param: [1.4550060033798218, 2.8938894271850586, 4.102473258972168, 3.9424173831939697], weights: [0.3228561282157898, 0.5516794919967651, 0.1254643350839615], train_wt_loss:  0.7399, val_wt_loss: 0.6550, train_grp_loss: [15.423103332519531, 17.1556453704834, 14.57862663269043], val_grp_loss: [17.476787567138672, 16.62436866760254, 16.258180618286133], max_reward_err: 0.0218, max_reward_err_index: 0, max_kl_dist: 0.1112, max_kl_dist_index: 0, max_train_grp_loss: 17.1556, max_train_grp_loss_index: 1, max_val_grp_loss: 17.4768, max_val_grp_loss_index: 0
2024-10-23 01:01:22,023 - /Users/noesis/Code/group-robust-preference-optimization/algos/linear_bandit/common_gradient_descent.py[line:386] - INFO: Iteration: 8, train_loss: 0.6360, val_loss: 0.6407, grad_norm:  1.1982, live_grad: 0.0000, reward_err: 0.0215, 0.0215, 0.0215, KL_dist: 0.1126, 0.1126, 0.1126, param: [1.5517512559890747, 2.9308526515960693, 4.1468892097473145, 3.9831349849700928], weights: [0.3145560324192047, 0.5767141580581665, 0.10872980207204819], train_wt_loss:  0.7471, val_wt_loss: 0.6552, train_grp_loss: [15.269393920898438, 17.012659072875977, 14.472308158874512], val_grp_loss: [17.338966369628906, 16.489980697631836, 16.11607551574707], max_reward_err: 0.0215, max_reward_err_index: 0, max_kl_dist: 0.1126, max_kl_dist_index: 0, max_train_grp_loss: 17.0127, max_train_grp_loss_index: 1, max_val_grp_loss: 17.3390, max_val_grp_loss_index: 0
2024-10-23 01:01:22,846 - /Users/noesis/Code/group-robust-preference-optimization/algos/linear_bandit/common_gradient_descent.py[line:386] - INFO: Iteration: 9, train_loss: 0.6350, val_loss: 0.6398, grad_norm:  1.1821, live_grad: 0.0000, reward_err: 0.0206, 0.0206, 0.0206, KL_dist: 0.1142, 0.1142, 0.1142, param: [1.6474815607070923, 2.9671552181243896, 4.190492153167725, 4.023015022277832], weights: [0.305289626121521, 0.6008698344230652, 0.0938405692577362], train_wt_loss:  0.7534, val_wt_loss: 0.6553, train_grp_loss: [15.120718955993652, 16.873437881469727, 14.370601654052734], val_grp_loss: [17.205154418945312, 16.359594345092773, 15.978321075439453], max_reward_err: 0.0206, max_reward_err_index: 0, max_kl_dist: 0.1142, max_kl_dist_index: 0, max_train_grp_loss: 16.8734, max_train_grp_loss_index: 1, max_val_grp_loss: 17.2052, max_val_grp_loss_index: 0
2024-10-23 01:01:23,712 - /Users/noesis/Code/group-robust-preference-optimization/algos/linear_bandit/common_gradient_descent.py[line:386] - INFO: Iteration: 10, train_loss: 0.6339, val_loss: 0.6388, grad_norm:  1.1661, live_grad: 0.0000, reward_err: 0.0204, 0.0204, 0.0204, KL_dist: 0.1161, 0.1161, 0.1161, param: [1.7421470880508423, 3.00285005569458, 4.233321189880371, 4.062110424041748], weights: [0.2952442467212677, 0.6240870952606201, 0.0806686207652092], train_wt_loss:  0.7588, val_wt_loss: 0.6554, train_grp_loss: [14.97695255279541, 16.73790740966797, 14.273337364196777], val_grp_loss: [17.07526397705078, 16.23311996459961, 15.844819068908691], max_reward_err: 0.0204, max_reward_err_index: 0, max_kl_dist: 0.1161, max_kl_dist_index: 0, max_train_grp_loss: 16.7379, max_train_grp_loss_index: 1, max_val_grp_loss: 17.0753, max_val_grp_loss_index: 0
